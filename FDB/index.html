<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Frame Difference-Based Temporal Loss for Video Stylization</title>
	<meta property="og:image" content="./resources/teaser.png"/> 
	<meta property="og:title" content="Frame Difference-Based Temporal Loss for Video Stylization." />
	<meta property="og:description" content="We propose a simple temporal loss to stablize style-transferred videos." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Frame Difference-Based Temporal Loss for Video Stylization</span>
		<table align=center width=900px>
			<table align=center width=900px>
				<tr>
					<td align=center width=300px>
						<span style="font-size:24px"><a href="https://atlantixjj.github.io">Jianjin Xu</a><sup>1,2</sup></span>
					</td>
					<td align=center width=300px>
						<span style="font-size:24px"><a href="http://www.cs.columbia.edu/~cxz/">Zheyang Xiong</a><sup>3</sup></span>
					</td>
					<td align=center width=300px>
						<span style="font-size:24px"><a href="http://www.cs.columbia.edu/~cxz/">Xiaolin Hu</a><sup>1</sup></span>
					</td>
				</tr>
				<tr>
					<td align=center width=200px>
						<span style="font-size:24px"><sup>1</sup>Tsinghua University</span>
					</td>
					<td align=center width=200px>
						<span style="font-size:24px"><sup>2</sup>Panzhihua University</span>
					</td>
					<td align=center width=200px>
						<span style="font-size:24px"><sup>2</sup>Rice University</span>
					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<span style="font-size:24px"><a href='https://arxiv.org/abs/2102.05822'>[Paper]</a></span>
					</td>
					<td align=center width=120px>
						<span style="font-size:24px"><a href='https://github.com/AtlantixJJ/frame-difference-loss'>[Code]</a></span><br>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<!-- no teaser for FDB
	<center>
		<table align=center width=850px>
			<tr>
				<td align=center>
					<img class="round" style="width:500px" src="./resources/teaser.png"/>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					We find that the generated image semantics can be extracted from GAN's feature maps using a linear transformation.
				</td>
			</tr>
		</table>
	</center>-->

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				<p>Neural style transfer models have been used to stylize an ordinary video to specific styles. To ensure temporal
				inconsistency between the frames of the stylized video, a common approach is to estimate the optic flow of the pixels in the original
				video and make the generated pixels match the estimated optical flow. This is achieved by minimizing an optical flow-based (OFB) loss
				during model training. However, optical flow estimation is itself a challenging task, particularly in complex scenes. In addition, it incurs a
				high computational cost. We propose a much simpler temporal loss called the frame difference-based (FDB) loss to solve the temporal
				inconsistency problem. It is defined as the distance between the difference between the stylized frames and the difference between the
				original frames. The differences between the two frames are measured in both the pixel space and the feature space specified by the
				convolutional neural networks. A set of human behavior experiments involving 62 subjects with 25,600 votes showed that the
				performance of the proposed FDB loss matched that of the OFB loss. The performance was measured by subjective evaluation of
				stability and stylization quality of the generated videos on two typical video stylization models. The results suggest that the proposed
				FDB loss is a strong alternative to the commonly used OFB loss for video stylization.
				</p>
			</td>
		</tr>
	</table>
	<br>

	<hr>

	<table align=center width=800px>
		<center><h1>Paper and Supplementary Materials</h1></center>
		<tr>
			<td><a href="resources/Linear_Semantics_in_GAN.pdf"><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>

			<td><span style="font-size:14pt">Jianjin Xu, Zheyang Xiong, Xiaolin Hu<br>
				<b>Frame Difference-Based Temporal Loss for Video Stylization</b><br>
				Submitted to TPAMI.<br>
				(hosted on <a href="https://arxiv.org/abs/2104.00487">ArXiv</a>)
				<br>
				<span style="font-size:14pt">
					<a href="./resources/bibtex.txt">
						[Bibtex]
					</a>
				</span>
			</td>

		</tr>
	</table>
	<br>
	<br>

	<hr>
	<center><h1>Methods</h1></center>

	<table align=center width=850px>
		<tr>
			<td align=center>
				<img class="round" style="width:850px" src="./resources/pipeline.png"/>
			</td>
		</tr>
		<tr>
			<td>
				<p>
				We propose a Frame Difference-Based (FDB) loss for stablizing stylized videos.
				The FDB loss is the distance between the difference of original frames and the difference of stylized frames.
				The difference is calculated in both pixel space and feature space.
				</p>
				<br>
			</td>
		</tr>
		<tr>
			<td align=center>
				<img class="round" style="width:400px" src="resources/cfdb_stability.png"/>
				<img class="round" style="width:400px" src="resources/cfdb_quality.png"/>
			</td>
		</tr>
		<tr>
			<td>
				<p>We evaluate the proposed FDB to the Optic Flow-Based (OFB) loss, which is the baseline.
				As the instability phenomena is subtle in pixel values, existing numeric metrics fail to discriminate the stablization ability of different algorithms, we resort to user study to evaluate the video stability and frame quality of our methods.</p>
				<br>
				<p>We conduct two-alternative forced choice (2AFC) experiments on self-hosted behavior studies.
				The user study involved a total of 62 subjects and 25,600 votes.
				Results show that the proposed loss (C-FDB loss) is able to match the OFB loss in both frame quality and video stability.
				</p>
			</td>
		</tr>
	</table>


	<hr>

	<center><h1>Applications</h1></center>

	<table align=center width=850px>
		<tr>
			<td align=center>
				<video controls="controls" height="400px" loop="loop" autoplay='autoplay' src='resources/LinearGAN_Training.mp4' type="video/mp4">
					Your browser does not support the video element.
					</video>
			</td>
		</tr>
		<tr>
			<td align=center>
				<p><i>Video: demonstration of training a few-shot LSE.</i></p>
			</td>
		</tr>
		<tr>
			<td>
				<p>After annotating a few annotations, we obtain a competitve semantic segmentation model for GAN generated images. The few-shot LSEs further enables image editing applications.</p>
				<br>
			</td>
		</tr>
	</table>

	<table align=center width=850px>
		<tr>
			<td>
				<p>We propose two image editing applications:
				few-shot <strong>Semantic Image Editing</strong> and few-shot <strong>Semantic Conditional Sampling</strong>.
				Both applications are formulated as an optimization problem:
				find a latent vector that produces images satisfying given semantic constraints.
				For SIE, we optimize the initial latent vector to get close to the user-modified semantic mask.
				For SCS, we optimize several random latent vectors and return the closest one to the user-specific semantic mask.
				</p>
				<br>
			</td>
		</tr>
		
		<tr>
			<td>
				<img class="round" src="resources/SIE_stroke.png" style="width: 850px"/>
			</td>
		</tr>
		<tr>
			<td align=center>
				<p><i>Figure: comparison of color-based editing, SIE using UNet (baseline), SIE using 8-shot LSE (ours), and SIE using fully trained LSE.</i></p>
				<br>
			</td>
		</tr>

		<tr>
			<td>
				<p><strong>Semantic Image Editing</strong>.
				In many cases, the user may want to control a GAN’s image generation process. For example, they might want to adjust the hair color of a generated facial image from blond to red; and the user may draw a red stroke on the hair to easily specify their intent.
				However, without explicit notion of semantics, the minimization process may not respect image semantics, leading to undesired changes of shapes and textures.
				Leveraging LSE, we propose an approached called Semantic Image Editing (SIE) to enable semantic-aware image generation.
				Our method can edit the images better than the baseline, SIE(UNet).
				More results are shown in the video below.
				</p>
				<br>
			</td>
		</tr>
		<tr>
			<td>

			</td>
		</tr>
	</table>

	<br>
	<hr>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

