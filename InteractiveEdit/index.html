<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Frame Difference-Based Temporal Loss for Video Stylization</title>
	<meta property="og:image" content="./resources/teaser.png"/> 
	<meta property="og:title" content="Frame Difference-Based Temporal Loss for Video Stylization." />
	<meta property="og:description" content="We propose a simple temporal loss to stablize style-transferred videos." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Frame Difference-Based Temporal Loss for Video Stylization</span>
		<table align=center width=900px>
			<table align=center width=900px>
				<tr>
					<td align=center width=300px>
						<span style="font-size:24px"><a href="https://atlantixjj.github.io">Jianjin Xu</a></span>
					</td>
					<td align=center width=300px>
						<span style="font-size:24px"><a href="https://github.com/equation314">Yuekai Jia</a></span>
					</td>
					<td align=center width=300px>
						<span style="font-size:24px">Mingyang Kou</span>
					</td>
					<td align=center width=300px>
						<span style="font-size:24px"><a href="http://www.cs.columbia.edu/~cxz/">Xiaolin Hu</a></span>
					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center>
						<span style="font-size:24px">Tsinghua University</span>
					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<span style="font-size:24px"><a href='https://arxiv.org/abs/2102.05822'>[Paper]</a></span>
					</td>
					<td align=center width=120px>
						<span style="font-size:24px"><a href='https://github.com/AtlantixJJ/frame-difference-loss'>[Code]</a></span><br>
					</td>
				</tr>
			</table>
		</table>
	</center>


	<center>
		<table align=center width=850px>
			<tr>
				<td align=center>
					<video controls="controls" width="800px" loop="loop" autoplay='autoplay' src='resources/media1.mp4' type="video/mp4">
						Your browser does not support the video element.
						</video>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					We find that the generated image semantics can be extracted from GAN's feature maps using a linear transformation.
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				<p>Neural style transfer models have been used to stylize an ordinary video to specific styles. To ensure temporal
				inconsistency between the frames of the stylized video, a common approach is to estimate the optic flow of the pixels in the original
				video and make the generated pixels match the estimated optical flow. This is achieved by minimizing an optical flow-based (OFB) loss
				during model training. However, optical flow estimation is itself a challenging task, particularly in complex scenes. In addition, it incurs a
				high computational cost. We propose a much simpler temporal loss called the frame difference-based (FDB) loss to solve the temporal
				inconsistency problem. It is defined as the distance between the difference between the stylized frames and the difference between the
				original frames. The differences between the two frames are measured in both the pixel space and the feature space specified by the
				convolutional neural networks. A set of human behavior experiments involving 62 subjects with 25,600 votes showed that the
				performance of the proposed FDB loss matched that of the OFB loss. The performance was measured by subjective evaluation of
				stability and stylization quality of the generated videos on two typical video stylization models. The results suggest that the proposed
				FDB loss is a strong alternative to the commonly used OFB loss for video stylization.
				</p>
			</td>
		</tr>
	</table>
	<br>

	<hr>

	<table align=center width=800px>
		<center><h1>Paper and Supplementary Materials</h1></center>
		<tr>
			<td><a href="resources/Linear_Semantics_in_GAN.pdf"><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>

			<td><span style="font-size:14pt">Jianjin Xu, Zheyang Xiong, Xiaolin Hu<br>
				<b>Frame Difference-Based Temporal Loss for Video Stylization</b><br>
				Submitted to TPAMI.<br>
				(hosted on <a href="https://arxiv.org/abs/2104.00487">ArXiv</a>)
				<br>
				<span style="font-size:14pt">
					<a href="./resources/bibtex.txt">
						[Bibtex]
					</a>
				</span>
			</td>

		</tr>
	</table>
	<br>


<br>
</body>
</html>

