<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Linear Semantics in Generative Adversarial Networks</title>
	<meta property="og:image" content="./resources/teaser.png"/> 
	<meta property="og:title" content="Linear Semantics in Generative Adversarial Networks" />
	<meta property="og:description" content="We study how GAN model image semantics and thereby enable controllability over the semantics of generated images." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Linear Semantics in Generative Adversarial Networks</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://atlantixjj.github.io">Jianjin Xu</a><sup>1,2</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="http://www.cs.columbia.edu/~cxz/">Changxi Zheng</a><sup>1</sup></span>
						</center>
					</td>
				</tr>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><sup>1</sup>Columbia University</span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><sup>2</sup>Panzhihua University</span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2104.00487'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/AtlantixJJ/LinearGAN'>[Code]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td>
					<center>
						<img class="round" style="width:500px" src="./resources/teaser.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					We find that the generated image semantics can be extracted from GAN's feature maps using a linear transformation.
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Generative Adversarial Networks (GANs) are able to generate high-quality images, but it remains difficult to explicitly specify the semantics of synthesized images.
				In this work, we aim to better understand the semantic representation of GANs, and thereby enable semantic control in GAN's generation process.
				Interestingly, we find that a well-trained GAN encodes image semantics in its internal feature maps in a surprisingly simple way:
				<strong>a linear transformation of feature maps suffices to extract the generated image semantics.
				</strong>
			</td>
		</tr>

		<tr>
			<td>
			<br/>
			To verify this simplicity, we conduct extensive experiments on various GANs and datasets; and thanks to this simplicity, we are able to learn a semantic segmentation model for a trained GAN from a small number (e.g., 8) of labeled images.
			Last but not least, leveraging our findings, we propose two few-shot image editing approaches, namely Semantic-Conditional Sampling and Semantic Image Editing.
			Given a trained GAN and as few as eight semantic annotations, the user is able to generate diverse images subject to a user-provided semantic layout, and control the synthesized image semantics.
			</td>
		</tr>
	</table>
	<br>

	<hr>
	<center><h1>Talk</h1></center>
	<p align="center">
		<iframe width="960" height="540" src="https://www.youtube.com/embed/xcQqUJqu5WM" allow="autoplay; encrypted-media" frameborder="0" allowfullscreen></iframe>
	</p>

	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px"><a href=''>[Slides]</a>
				</span>
			</center>
		</tr>
	</table>

	<br>
	<hr>

	<table align=center width=700px>
		<center><h1>Paper and Supplementary Materials</h1></center>
		<tr>
			<td><a href="resources/Linear_Semantics_in_GAN.pdf"><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>

			<td><span style="font-size:14pt">Jianjin Xu, Changxi Zheng<br>
				<b>Linear Semantics in Generative Adversarial Networks</b><br>
				In CVPR 2021.<br>
				(hosted on <a href="https://arxiv.org/abs/2104.00487">ArXiv</a>)
				<br>
				<span style="font-size:14pt">
					<a href="./resources/bibtex.txt">
						[Bibtex]
					</a>
				</span>
			</td>
		</tr>
	</table>

	<br>
	<br>

	<hr>
	<center><h1>GAN's Linear Semantic Embedding</h1></center>

	<table align=center width=850px>
		<tr>
			<td align=center>
				<img class="round" style="width:800px" src="./resources/pipeline.png"/>
			</td>
		</tr>
		<tr>
			<td>
				<p><strong>Linear Semantic Extractor (LSE)</strong>. We find that the generated image semantics can be extracted from GAN's feature maps using a linear transformation.
				As shown in the figure above, the LSE simply upsamples and concatenates GAN's feature maps into a block, and then run a 1x1 convolution on top of the block.
				The LSE is trained using cross-entropy loss using supervisions provided by off-the-shelf segmentation network.
				We conduct extensive experiments to support the linear semantics hypothesis.
				Our evidence is two-fold.</p>
				<br>
			</td>
		</tr>

		<tr>
			<td align=center>
				<img class="round" style="width:850px" src="./resources/se_quant.png"/>
			</td>
		</tr>
		<tr>
			<td>
				<p><strong>Evidence 1: LSE suffices to extract the semantics from GANs</strong>.
				We study to what extent could the semantics be better extracted by Nonlinear Semantic Extractors (NSEs). 
				Table.1 shows that in most cases, the performance of LSE is within a range of 3.5% relative to the best NSE method.
				Although NSEs achieve the best performance, their differences with the LSEs are marginal.
				This experiment directly supports that LSE suffices to extract GAN's semantics.</p>
				<br>
			</td>
		</tr>

		<tr>
			<td align=center>
				<img style="width:450px" src="./resources/fewshot_quant.png"/>
			</td>
		</tr>
		<tr>
			<td>
				<p><strong>Evidence 2: LSE can be trained using few annotations</strong>.
				We only use N=1, 4, 8, 16 annotations to train the LSE (for StyleGAN2 models) and repeatedly train each LSE 5 times to account for the training data variance.
				As shown in Table.2, given N=16 annotations, the 16-shot LSEs achieve around 90%, 80%, and 70% performance relative to the fully trained models.</p>
				<br>
			</td>
		</tr>
	</table>

	<!--<table align=center width=900px>
		<tr>
			<td align=center>
				<img class="round" style="width: 300px" src="resources/face_appendix_5.png" />
			</td>
			<td align=center>
				<img class="round" style="width: 300px" src="resources/bedroom_appendix_5.png" />
			</td>
			<td align=center>
				<img class="round" style="width: 300px" src="resources/church_appendix_5.png" />
			</td>
		</tr>
	</table>-->

	<table align=center width=900px>
		<tr>
			<td align=center>
				<img class="round" style="width: 300px" src="resources/face_paper_1.png" />
			</td>
			<td align=center>
				<img class="round" style="width: 300px" src="resources/bedroom_paper_1.png" />
			</td>
			<td align=center>
				<img class="round" style="width: 300px" src="resources/church_paper_1.png" />
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<tr>
			<td align=center>
				<p><i>Figure: qualitative comparison of LSE, NSE-1, and NSE-2.</i></p>
				<br>
			</td>
		</tr>
	</table>

	<hr>

	<center><h1>Applications</h1></center>

	<table align=center width=850px>
		<tr>
			<td align=center>
				<video controls="controls" height="400px" loop="loop" autoplay='autoplay' src='resources/LinearGAN_Training.mp4' type="video/mp4">
					Your browser does not support the video element.
					</video>
			</td>
		</tr>
		<tr>
			<td align=center>
				<p><i>Video: demonstration of training a few-shot LSE.</i></p>
			</td>
		</tr>
		<tr>
			<td>
				<p>After annotating a few annotations, we obtain a competitve semantic segmentation model for GAN generated images. The few-shot LSEs further enables image editing applications.</p>
				<br>
			</td>
		</tr>
	</table>

	<table align=center width=850px>
		<tr>
			<td>
				<p>We propose two image editing applications:
				few-shot <strong>Semantic Image Editing</strong> and few-shot <strong>Semantic Conditional Sampling</strong>.
				Both applications are formulated as an optimization problem:
				find a latent vector that produces images satisfying given semantic constraints.
				For SIE, we optimize the initial latent vector to get close to the user-modified semantic mask.
				For SCS, we optimize several random latent vectors and return the closest one to the user-specific semantic mask.
				</p>
				<br>
			</td>
		</tr>
		
		<tr>
			<td>
				<img class="round" src="resources/SIE_stroke.png" style="width: 850px"/>
			</td>
		</tr>
		<tr>
			<td align=center>
				<p><i>Figure: comparison of color-based editing, SIE using UNet (baseline), SIE using 8-shot LSE (ours), and SIE using fully trained LSE.</i></p>
				<br>
			</td>
		</tr>

		<tr>
			<td>
				<p><strong>Semantic Image Editing</strong>.
				In many cases, the user may want to control a GAN’s image generation process. For example, they might want to adjust the hair color of a generated facial image from blond to red; and the user may draw a red stroke on the hair to easily specify their intent.
				However, without explicit notion of semantics, the minimization process may not respect image semantics, leading to undesired changes of shapes and textures.
				Leveraging LSE, we propose an approached called Semantic Image Editing (SIE) to enable semantic-aware image generation.
				Our method can edit the images better than the baseline, SIE(UNet).
				More results are shown in the video below.
				</p>
				<br>
			</td>
		</tr>
		<tr>
			<td>
				<p align="center">
					<iframe width="960" height="540" src="https://www.youtube.com/embed/vIrnxl2L36U" allow="autoplay; encrypted-media" frameborder="0" allowfullscreen></iframe>
				</p>
			</td>
		</tr>
		<tr>
			<td align=center>
				<i>
					Video: more Semantic Image Editing results using our web application.
					<br>
					<br>				</i>
				<br>
			</td>
		</tr>

		<tr>
			<td>
				<p><strong>Semantic Conditional Sampling (SCS)</strong> aims to synthesize an image subject to a semantic mask.
				The baseline for SCS is to use a pretrained segmentation network.
				The proposed method is to use few-shot LSEs.
				The comparisons are shown below.
				</p>
			</td>
		</tr>
	</table>

	<table align=center width=850px>
		<tr>
			<td align=center>
				<img src="./resources/scs_ffhq_baseline.png" width=450px />
			</td>
			<td align=center>
				<img src="./resources/scs_church_baseline.png" width=450px />
			</td>
		</tr>
		<tr>
			<td align=center>
				<i>SCS on FFHQ using UNet (baseline).</i>
			</td>
			<td align=center>
				<i>SCS on Church using DeepLabV3 (baseline).</i>
			</td>
		</tr>
		<tr>
			<td align=center>
				<img src="./resources/scs_ffhq_8.png" width=450px />
			</td>
			<td align=center>
				<img src="./resources/scs_church_8.png" width=450px />
			</td>
		</tr>
		<tr>
			<td align=center>
				<i>SCS on FFHQ using 8-shot LSE (ours).</i>
			</td>
			<td align=center>
				<i>SCS on Church using 8-shot LSE (ours).</i>
			</td>
		</tr>
	</table>

	<br>
	<hr>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

