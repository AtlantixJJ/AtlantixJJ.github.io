---
layout: post
title:  "Vehicle Re-Identification in Unrestricted Scenario"
date:   2018-11-08 17:03:23 +0800
categories: jekyll update
---

# Introduction

Vehicle ReId by deep learning is still a new practice, with only limited data provided, like VeRi-776, VID, and CompCars. Current research bases on these two datasets, now reaching the accuracy of around 90% CMC@1 in VeRi and around 70% CMC@1 in VID. However, in CVPR 2018 workshop, NVIDIA held an AI city challenge containing a track of vehicle retrieval, and the results were really poor, which was approximately 30%. The main reason for low performance is that the dataset is under unrestricted practical scene. The challenge's dataset is several 90 minute long raw video of traffic monitors, as compared to those cropped and cleaned images of vehicles in those dataset.

Possible cause of performance degradation is not simply cross dataset generalization, but also the complicated syndrome emerged with the combination of detection, tracking, matching and enormous problems in practice. In this article I describe my work on vehicle re-id under unrestricted scenrario during internship at MSRA. It's pitiful that I did not make any novel breakthrough in only 3-months internship, but I managed to set up a well-developed framework for unrestricted vehicle re-id, and tried some methods to improve the performance.

My system is simple in framework. For each vehicle, it first detects, and then tracks within the same camera. When vehicles disappear in the video, it then try to match the vehicle to already identified vehicle (retrieved) or become a new item in database. Detection is done basically by faster RCNN, implemented in Caffe2/Detectron. In camera tracking and cross camera matching all rely on a learned deep metric. 

The system looks like the following:

{% assign video_url="https://github.com/AtlantixJJ/atlantixjj.github.io/blob/master/assets/vere/veri_b2_ex2.mp4?raw=true" %}
{% include video.html %}

This is a segment of re-id system demo. (Watch in full screen is recommended.)

The left part of video is from two monitors. Next to them are two round queue displaying the vehicle detected and tracked independently at each camera. And the rightmost part is the retrieval window, with a retrieval distance indicating their similarity (the lower the better).

Actually the monitors are set on two sides of a bridge. As vehicles pass by, they first enter surveillance region from cam 0 and then exit from cam 1. The upper one is cam 0 and the lower one is cam 1, which are regarded as retrieval source and query source respectively. So I expect the system to retrieve every vehicle in cam 1 from cam 0, because there is no other side way for vehicles to disappear. However, this task is very difficult because vehicles in cam 0 is from a front view and there are only back view in cam 1.

As demonstrated in the video, the yellow vehicle came along in upper video screen, and stay tracked in the orange box. An sequentially assigned id indicates a tracklet identified in the same camera. You may observe that the detection box is flickering. This means that at some frame our detector fails to detect this vehicle, and in this case my tracking algorithm keeps the vehicle tracked. After the yellow vehicle left cam 0, it immediately appear in the round queue display of cam 0. A few seconds later, it appears in cam 1. When it disappears, it is added to the display round queue of cam 1 and a matching successfully retrieved itself in cam 0. Another taxi is also successfully retrieved.

Notice that, our system is completely based on appearance, and is irrelevant with time. Therefore, although the cameras are set very near, there is no much difference with cameras set far away.

# Core implementation

The core part of this system is a learned deep metric. My work is based on Deep Relative Distance Learning, a simple method to learn vehicle distance. It is rather tricky that the largest improvement of this baseline model is to substitude ResNet for original VGG. This is enough to obtain a state of the art performance (at July. 2018), well, because this field is really underdeveloped.

I am keen on analysis. Through statistics and some visualization, I identified a bottleneck of current method, that vehicle from similar view is likely to confuse neural network, even if they are so different in human perception. I alleviate this problem by improving sampling proportion in triplet loss and do something like hard example mining.

Next I confirmed by visualization and analysis that detail part is what the model fails to utilize. There is a strange phenomena that human is able confident to assert two cars to be the same, especially when the view is different. Quite oppositely, deep learning model is able to assert two cars to be the same, but fails to identify rather easy mismatches. 

![]({{site.baseurl}}/assets/vere/analysis.png)

The figure above is obtained in VeRi dataset. The left column is query image, and the rest columns are retrieved by trained deep metric. You can try to retrieve vehicles and compare to machine!

After identifying this problem, I worked on several methods trying to solve it. 

First, I tried to learn a multiscale attention, focusing on those detail part. An additional attention branch is added to the output of each residual block, either generated by itself or from higher level. Like the architecture below. But the performance is worse than original model. Not only do they achieve less in training phase, but also generalize worse in validation dataset. This is probably because dataset does not provide sufficient information. For example, there is no perfect correspondence of different vehicle in the same position and view, so that the network can compare the different of the existence of detailed part. Also there is even no view information available. After a few weeks' experiment, I turned to less explicit method. The model architecture:

![]({{site.baseurl}}/assets/vere/attention_model.png)

Second, I observe and infer that high level feature of ResNet is likely to have the information of detailed part, i.e. whether the part is minor or not, its information is preserved rather than disgarded. I show this by a visulization experiment: appending a visualization layer to the output of some layers and require them to restore the origin image. 

![]({{site.baseurl}}/assets/vere/draw_net.png)

When training visualization layer, the main model is not trained, i.e. the visualization layer does not influenece normal training. The result is shown in the following figure:

![]({{site.baseurl}}/assets/vere/vis.png)

The strange yellow color is just overflowed color due to some bad code, please ignore. As the layer goes deeper, the resolution gets lower, but the detail part is still visible. So I guess their infomation is stored in those high dimensional vector in deep features. So instead of focusing on explict spatial attention, I could focus on something like channel-wise attention, which is expected to attend those detailed features inside the high dimensional feature. This is beneficial in two aspect: the model is simpler and easier to train.

However, the result is still worse than original model. The most probable reason is still the lack of data. The images in VeRi or VID is very different from each other, not similiar with the continuous frames in real world application. For the very same vehicle, the same detail feature may not persist. Some feature may be invisible at some view. We probably need a dense dataset, with more information from the nature of video to be exploited. Unfortunately there is no such dataset. But NVIDIA's challenge dataset is close to this objective.

# More demo

Another demo video with front-rear view pair, where the situation is much more complicate:

{% assign video_url="https://github.com/AtlantixJJ/atlantixjj.github.io/blob/master/assets/vere/veri_b2_ex1.mp4?raw=true" %}
{% include video.html %}

I also conduct an experiment in same view vehicle retrieval. The camera is set on two bridges along the same road.

![]({{site.baseurl}}/assets/vere/veri_b3_1.png)

![]({{site.baseurl}}/assets/vere/veri_b3_4.png)

