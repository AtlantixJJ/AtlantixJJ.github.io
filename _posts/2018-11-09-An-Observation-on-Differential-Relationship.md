---
layout: post
title:  "An Observation on Differential Relationship"
date:   2018-11-09 08:00:00 +0800
categories: jekyll update
---

I found a phemonena during developing the previous project ![Neural Painter](https://atlantixjj.github.io/jekyll/update/2018/09/01/NeuralPainter.html) that when I want to change the hair color of an anime character, I only need to stroke a single color point on the hair region, and then the whole hair region will change color as a whole. As shown in the following figure, a blue square on the subject's hair direct GAN to morph the hair color to given color. The next few anime faces are generated by small learning step, which we can see a gradual change toward the target hair color while the other part stays the same. The next few heatmaps show the pixel different, reflecting more clearly of the color change.

![]({{site.baseurl}}/assets/differential_relationship.png)

This is expected actually, because a partial color change would be unrealistic and GAN should not learn it. And similar things happens when we modify eye color and skin color. We expect GAN to learn this, so that user can modify the image with minimal effort.

But this phemona also confirms that GAN does model the semantic information during the transformation of noise. A typical DCGAN transforms noise input structured data like image with less obvious structure to represent the structured information. The detailed appearance information and semantic control information mix in the internal representation of GAN. However in practice I found that the semantic information does exist, otherwise the whole region of hair would not be changed by a edit loss on a sub region.

This leads me to think about enhancing the semantic representation of GAN, in order to interpret the behavior of GAN, or just simply improving the generation quality. My proposal is a new kind of penalty, to penalize the change outside semantic region under current edit. As shown in the figure above, there are also undesired change of appearance in the subject, in the skin and clothes. I propose the penalize this kind of undesired change, providing addition prior to guide the evolution of better GAN feature.

Moreover, I discover this is actually a relationship between output units -- differential relationship. I searched on the web and had not found something close to my indication, so I just invent this word. It means that a set of visible variable is related to each other by some latent mechanism, which kind of describes a joint distribution of these variables. To reveal the relationship between these variables, we can do the following experiment: change a variable a little bit, and then by some latent mechanism all other variable change as well, and these changes might tell us interesting things. For example, in GAN cases, units belong to the same semantic region might well change together and units on other regions won't.